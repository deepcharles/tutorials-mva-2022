{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Machine Learning for Time Series (Master MVA)**\n",
    "\n",
    "- Tutorial 2, Friday 21<sup>st</sup> January 2022\n",
    "- [Link to the class material.](http://www.laurentoudre.fr/ast.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this tutorial, we illustrate the following concepts:\n",
    "\n",
    "- feature extraction on time series\n",
    "- feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import tee\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from loadmydata.load_uea_ucr import load_uea_ucr_data\n",
    "from numpy.fft import rfft, rfftfreq\n",
    "from scipy.cluster import hierarchy\n",
    "from scipy.signal import argrelmax, stft\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.stats import f_oneway, spearmanr\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from statsmodels.tsa.stattools import acf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Utility functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise(iterable):\n",
    "    \"s -> (s0,s1), (s1,s2), (s2, s3), ...\"\n",
    "    a, b = tee(iterable)\n",
    "    next(b, None)\n",
    "    return zip(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_largest_local_max(signal1D: np.ndarray, order: int = 1):\n",
    "    \"\"\"Return the largest local max and the associated index in a tuple.\n",
    "\n",
    "    This function uses `order` points on each side to use for the comparison.\n",
    "    \"\"\"\n",
    "    all_local_max_indexes = argrelmax(signal1D, order=order)[0]\n",
    "    all_local_max = np.take(signal1D, all_local_max_indexes)\n",
    "    largest_local_max_index = all_local_max_indexes[\n",
    "        all_local_max.argsort()[-1]\n",
    "    ]\n",
    "\n",
    "    return signal1D[largest_local_max_index], largest_local_max_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fig_ax(figsize=(15, 5)):\n",
    "    return plt.subplots(figsize=figsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"BinaryHeartbeat\"\n",
    "data = load_uea_ucr_data(dataset_name)\n",
    "print(data.description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling frequency\n",
    "FREQUENCY = 2000  # Hz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose a signal from the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal = data.X_train[179].flatten()\n",
    "label = data.y_train[179]\n",
    "n_samples = signal.shape[0]\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = fig_ax()\n",
    "tt = np.arange(n_samples) / FREQUENCY\n",
    "ax.plot(tt, signal)\n",
    "_ = ax.set_xlim(0, n_samples / FREQUENCY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the spectrogram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, t, Zxx = stft(signal, fs=FREQUENCY, nperseg=1000, noverlap=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = fig_ax()\n",
    "ax.pcolormesh(t, f, np.abs(Zxx), vmin=0, shading=\"gouraud\")\n",
    "ax.set_xlabel(\"Time (s)\")\n",
    "_ = ax.set_ylabel(\"Frequency (Hz)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "    <p><b>Question</b></p>\n",
    "    <p>Replot the same spectrogram as above, but zoom on the interesting frequencies (set <tt>cut_off_freq</tt> in the following cell).</p>\n",
    "    <ul>\n",
    "        <li>What are the repeated patterns? What is the frequency of interest (approximately)?</li>\n",
    "        <li>What happens when you increase the number of samples per windows?</li>\n",
    "        <li>What happens when you increase the overlap between windows?</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cut-off frequency\n",
    "cut_off_freq = ...  # Hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = fig_ax()\n",
    "\n",
    "f, t, Zxx = stft(signal, fs=FREQUENCY, nperseg=1024, noverlap=512)\n",
    "\n",
    "keep_mask = f < cut_off_freq\n",
    "\n",
    "ax.pcolormesh(\n",
    "    t, f[keep_mask], np.abs(Zxx[keep_mask]), vmin=0, shading=\"gouraud\"\n",
    ")\n",
    "ax.set_xlabel(\"Time (s)\")\n",
    "_ = ax.set_ylabel(\"Frequency (Hz)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moments and percentiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distribution_features(signal: np.ndarray) -> dict:\n",
    "    res_dict = dict()\n",
    "    res_dict[\"mean\"] = signal.mean()\n",
    "    res_dict[\"std\"] = signal.std()\n",
    "    res_dict[\"min\"] = signal.min()\n",
    "    res_dict[\"max\"] = signal.max()\n",
    "    return res_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_distribution_features(signal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "    <p><b>Question</b></p>\n",
    "    <p> Rewrite in the following cell, the function <tt>get_distribution_features(signal)->dict</tt> so that is also computes the kurtosis, the skew (available in the <tt>scipy.stats</tt> module) and the 25%, 50% and 75% percentiles.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distribution_features(signal: np.ndarray) -> dict:\n",
    "    res_dict = dict()\n",
    "    res_dict[\"mean\"] = signal.mean()\n",
    "    res_dict[\"std\"] = signal.std()\n",
    "    res_dict[\"min\"] = signal.min()\n",
    "    res_dict[\"max\"] = signal.max()\n",
    "\n",
    "    return res_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autocorrelation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a signal $x\\in\\mathbb{R}^N$ with $N$ samples, the autocorrelation with lag $m$ is defined as follows:\n",
    "\n",
    "$$\n",
    "\\hat{\\gamma}[m] := \\frac{1}{N-|m|} \\sum_{n=0}^{N-1} x[n]x[n+m].\n",
    "$$\n",
    "\n",
    "Note that if the signal $x$ is periodic, then the autocorrelation is also periodic with same period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us plot the autocorrelation of a sound signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = fig_ax()\n",
    "ax.plot(acf(signal, nlags=200, fft=True), \".-\")\n",
    "ax.axhline(0, ls=\"--\", color=\"k\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "    <p><b>Question</b></p>\n",
    "    <p>We would like to also compute the maximum autocorrelation value and the associated lag in Hz (a local maximum that is not 0). Explain how you can convert a lag expressed in number of samples to a lag expressed in Hz.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the autocorrelation features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_autocorr_features(signal: np.ndarray, n_lags: int = 200) -> dict:\n",
    "    auto_corr = acf(signal, nlags=n_lags, fft=True)\n",
    "    res_dict = dict()\n",
    "    for (lag, auto_corr_value) in enumerate(auto_corr):\n",
    "        res_dict[f\"autocorrelation_{lag}_lag\"] = auto_corr_value\n",
    "\n",
    "    local_max, local_argmax = get_largest_local_max(auto_corr, order=20)\n",
    "    res_dict[\"lag_max_autocorrelation_Hz\"] = FREQUENCY / local_argmax\n",
    "    res_dict[\"max_autocorrelation\"] = local_max\n",
    "\n",
    "    return res_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_autocorr_features(signal, n_lags=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectral features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the Fourier coefficients of a signal (y=absolute value, x=frequency) using `rfft()` and `rfftfreq()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fourier = abs(rfft(signal)) ** 2\n",
    "freqs = rfftfreq(n=n_samples, d=1.0 / FREQUENCY)\n",
    "\n",
    "fig, ax = fig_ax()\n",
    "ax.plot(freqs, fourier)\n",
    "ax.set_xlabel(\"Frequency (Hz)\")\n",
    "_ = ax.set_ylabel(\"Fourier coefficient\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the spectral features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fourier_features(signal: np.ndarray, n_bins: int = 100) -> dict:\n",
    "    \"\"\"The signal is assumed to be centered and scaled to unit variance.\"\"\"\n",
    "\n",
    "    n_samples = signal.shape[0]\n",
    "    fourier = abs(rfft(signal))\n",
    "    freqs = rfftfreq(n=n_samples, d=1.0 / FREQUENCY)\n",
    "    res_dict = dict()\n",
    "\n",
    "    freq_bins = np.linspace(0, FREQUENCY / 2, n_bins + 1)\n",
    "    for (f_min, f_max) in pairwise(freq_bins):\n",
    "        keep = (f_min <= freqs) & (freqs < f_max)\n",
    "        res_dict[f\"fourier_{f_min:.1f}-{f_max:.1f}_Hz\"] = np.log(\n",
    "            np.sum(fourier[keep] ** 2)\n",
    "        )\n",
    "    return res_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_fourier_features(signal, n_bins=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "    <p><b>Question</b></p>\n",
    "    <p>The frequency features are not scaled. If the signal is normalized, do we need to?</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "    <p><b>Question</b></p>\n",
    "    <p> We have used regularly spaced frequency bins. What would be a better approach? </p>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate all features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we write a function `get_features(signal: np.ndarray) -> dict` that computes all features from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(signal: np.ndarray) -> dict:\n",
    "    res_dict = dict()\n",
    "\n",
    "    # stats\n",
    "    res_dict.update(get_distribution_features(signal))\n",
    "\n",
    "    # spectral\n",
    "    signal -= signal.mean()\n",
    "    signal /= signal.std()\n",
    "    res_dict.update(get_fourier_features(signal, n_bins=50))\n",
    "\n",
    "    # autocorrelation\n",
    "    res_dict.update(get_autocorr_features(signal, n_lags=200))\n",
    "\n",
    "    return res_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_features(signal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute all features over the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = pd.DataFrame(\n",
    "    [get_features(signal.data.flatten()) for signal in data.X_train]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "    <p><b>Question</b></p>\n",
    "    <p> How many features do we have in the end?</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Low variance threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = fig_ax(figsize=(20, 5))\n",
    "all_features.std().sort_values().plot(ax=ax)\n",
    "\n",
    "# change the height of the horizontal line here\n",
    "ax.axhline(0.01, ls=\"--\", color=\"k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "    <p><b>Question</b></p>\n",
    "    <p> Choose a variance threshold (name it <tt>variance_threshold</tt>).</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variance_threshold = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be careful when dropping low variance features, they might still be informative. Quickly check that it makes sense to drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_variance_features = all_features.std() < variance_threshold\n",
    "low_variance_features = low_variance_features[\n",
    "    low_variance_features\n",
    "].index.to_numpy()\n",
    "print(f\"There are {len(low_variance_features)} features to drop.\")\n",
    "print(low_variance_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell drops the low-variance features. Only execute it when you are sure of the features you want to remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features.drop(columns=low_variance_features, inplace=True, errors=\"ignore\")\n",
    "print(f\"There are {all_features.shape[1]} features left.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "    <p><b>Question</b></p>\n",
    "    <p> One feature has larger variations than the others: which one is it? (Name it <tt>feature_to_check</tt>.) Show the associated boxplot.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_to_check = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features[feature_to_check].plot(kind=\"box\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "    <p><b>Question</b></p>\n",
    "    <p> Check the outliers. Is there something we can do to remediate this issue?</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multicollinearity\n",
    "\n",
    "Multicollinearity degrades numerical stability and interpretability.\n",
    "Compute the rank-based Spearman correlation.\n",
    "Features with a correlation above a threshold are grouped together in a cluster.\n",
    "We then choose a single feature from each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_threshold = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = pdist(\n",
    "    all_features.to_numpy().T, metric=\"correlation\"\n",
    ")  # distance matrix\n",
    "plt.imshow(squareform(corr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = fig_ax((20, 10))\n",
    "corr_linkage = hierarchy.average(corr)\n",
    "dendro = hierarchy.dendrogram(\n",
    "    corr_linkage, ax=ax, color_threshold=1 - correlation_threshold\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the clusters\n",
    "cluster_ids = hierarchy.fcluster(\n",
    "    corr_linkage, 1 - correlation_threshold, criterion=\"distance\"\n",
    ")\n",
    "\n",
    "# print the largest clusters\n",
    "largest_cluster_ind = np.bincount(cluster_ids).argmax()\n",
    "print(\n",
    "    f\"The largest cluster is {all_features.columns[cluster_ids==largest_cluster_ind].tolist()}.\"\n",
    ")\n",
    "\n",
    "# for each cluster, only keep the first feature\n",
    "keep_features = list()\n",
    "for cluster in np.unique(cluster_ids):\n",
    "    cluster_indexes = np.where(cluster_ids == cluster)[0]\n",
    "    keep_features.append(all_features.columns[cluster_indexes[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce the number of features\n",
    "all_features = all_features[keep_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "    <p><b>Question</b></p>\n",
    "    <p> How many features do we have in the end?</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2, whiten=True).fit(all_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the 5 most important features (with highest norm).\n",
    "top_features_for_pca = np.linalg.norm(pca.components_, axis=0).argsort()[-5:]\n",
    "\n",
    "for feature_ind in top_features_for_pca:\n",
    "    msg = f\"{all_features.columns[feature_ind]}: {(pca.components_.T[feature_ind]**2).sum():.3f}\"\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the 2D projection of the top 5 features\n",
    "fig, ax = fig_ax(figsize=(8, 8))\n",
    "ax.axis(\"equal\")\n",
    "for feature_ind in top_features_for_pca:\n",
    "    dx, dy = pca.components_.T[feature_ind]\n",
    "    ax.plot([0, dx], [0, dy], label=all_features.columns[feature_ind])\n",
    "ax.set_xlabel(f\"PC1 ({pca.explained_variance_[0]:.1f}% of variance)\")\n",
    "ax.set_ylabel(f\"PC1 ({pca.explained_variance_[1]:.1f}% of variance)\")\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three main classes of methods:\n",
    "\n",
    "- **Filter methods.** We test the adequacy of the feature with the annotations, thanks to several criterion/scores (e.g. correlation).\n",
    "- **Wrapper methods.** We test the features on a supervised classification task, by trying several combinations. The best features are kept.\n",
    "- **Embedded methods.** Mixed approaches where we jointly infer the relevance of the features and classify the data (decision tree, sparse methods...)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter methods\n",
    "\n",
    "If a feature is highly correlated with a label, it can help achieve good classification performance.\n",
    "\n",
    "For classification tasks, ANOVA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = data.y_train\n",
    "statistics, pvalues = f_oneway(\n",
    "    all_features[labels == \"Abnormal\"], all_features[labels == \"Normal\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind in statistics.argsort()[::-1]:\n",
    "    print(f\"{all_features.columns[ind]}: {statistics[ind]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapper methods\n",
    "\n",
    "For a given learning algorithm, wrapper methods repeatedly select a subset of features and evaluate the selected features. Several procedures exist to select a subset but the most common are greedy and iterative; they either remove (backward selection) or add a feature (forward selection) sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At each steps, SequentialFeatureSelector adds the best scoring feature to\n",
    "# the set of selected features.\n",
    "# For a given estimator, the score is computed with cross-validation.\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "sfs = SequentialFeatureSelector(\n",
    "    knn, n_features_to_select=20, cv=5, scoring=make_scorer(accuracy_score)\n",
    ")\n",
    "sfs.fit(all_features, labels)\n",
    "\n",
    "print(\"Selected features:\")\n",
    "print(all_features.columns[sfs.get_support()].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "    <p><b>Question</b></p>\n",
    "    <p>Run the same selection as above, but with backward selection (<tt>sklearn.feature_selection.RFE</tt>).</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedded methods\n",
    "\n",
    "Such methods use the intrinsic structure of a learning algorithm to embed feature selection into the underlying model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier().fit(all_features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "    <p><b>Question</b></p>\n",
    "    <p>Print features in descending order of importance, measured by the random forest feature importance (see the <tt>feature_importances_</tt> attribute).</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
